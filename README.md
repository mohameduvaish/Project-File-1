# Credit Risk Prediction with SHAP Explainability

This project builds and explains a machine learning model for **credit risk prediction**, using SHAP (SHapley Additive exPlanations) to provide transparent and interpretable insights about how features influence model decisions.

The goal is to help understand **why** a borrower is classified as high-risk or low-risk, ensuring fairness, transparency, and trust in automated decision-making systems.

---

## ğŸ“‚ Project Structure

```text
Project-File-1/
â”‚
â”œâ”€â”€ Untitled11.ipynb                 # Main executed notebook (model + SHAP analysis)
â”œâ”€â”€ credit_risk.csv                  # Dataset used for training and evaluation
â”œâ”€â”€ shap_summary_plot.png            # Global SHAP feature importance
â”œâ”€â”€ shap_values_output.csv           # SHAP values exported from notebook
â”œâ”€â”€ report.md                        # Detailed analysis and interpretation
â”œâ”€â”€ requirements.txt                 # Libraries used in the notebook
â””â”€â”€ README.md                        # Project documentation (this file)
```

---

## ğŸš€ Project Workflow

### 1ï¸âƒ£ Data Loading & Preprocessing
- Loads the `credit_risk.csv` dataset  
- Handles missing values  
- Encodes categorical variables  
- Splits data into train/test sets  

### 2ï¸âƒ£ Model Training
A machine learning classifier (Logistic Regression / Random Forest / XGBoost depending on the notebook) is used to predict credit default risk.

### 3ï¸âƒ£ Model Evaluation
The notebook reports key metrics such as:

- Accuracy  
- Precision  
- Recall  
- F1-score  
- Confusion matrix  

These help evaluate overall predictive performance.

### 4ï¸âƒ£ Explainability with SHAP
SHAP is used to provide interpretability:

#### ğŸ”¹ Global Interpretability
- `shap_summary_plot.png`  
  - Shows the most important features affecting model predictions  
  - Highlights direction (positive/negative) and magnitude of influence  

#### ğŸ”¹ Local Interpretability
- Individual-level SHAP values (from `shap_values_output.csv`)  
  - Explain why a specific person was classified high-risk or low-risk  
  - Useful for fairness and regulatory compliance  

---

## ğŸ“Š Outputs

All important outputs generated by the notebook include:

- **Global SHAP Summary Plot**
- **SHAP Values CSV**
- **Model performance metrics**
- **Confusion matrix visualizations** (if included in notebook)
- **Written interpretation in `report.md`**

---

## ğŸ§  Purpose of the Project

This project demonstrates:

- How credit risk models operate  
- How explainability tools (SHAP) help interpret model predictions  
- How to identify important risk factors  
- How to justify model decisions to stakeholders and auditors  

This is crucial in financial applications where decisions must be transparent and accountable.

---

## ğŸ’» How to Run

1. Install required dependencies:
```bash
pip install -r requirements.txt
```

2. Open the notebook:
```
Untitled11.ipynb
```

3. Run all cells to reproduce:
- Model training  
- Evaluation metrics  
- SHAP feature interpretations  
- Plot exports  

---

## ğŸ“„ Additional Documentation

See **report.md** for:

- Full interpretation of results  
- Explanation of SHAP values  
- Strengths & limitations of the model  
- Recommendations for improvement  

---

## ğŸ‘¤ Author

**Mohamed Uvaish**  
GitHub: https://github.com/mohameduvaish
